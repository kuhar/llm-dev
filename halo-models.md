
# Introduction
This page is project tracker to get halo models like llama3, Flux.1, Mistral etc. working on one or more MI3xx using shark/iree. 

# Release Goals
- Shark V3.1 (Jan 2025) llama3.1 405B sharded across 8 MI300x GPUs performant at level of vLLM PyTorch (Fused Ops Eager Mode)
- Shark V3.2 (Feb 2025) Flux.1dev and Mixtral 8x7B performant at 125% of PyTorch-rocm

# Glossary

TPn: Tensor Parallel using n GPUs where a large tensor is sharded across multiple GPUs using sharktank and scatter/gather to/from GPUs is done in single MLIR

TTFT: Time To First Token (time taken from processing of prompt to first token generated by the prefill stage)

ITL: Average time between each new token generated in decode phase (second token onwards)

# Run Instructions
- [shortfin SDXL](https://github.com/nod-ai/SHARK-Platform/tree/main/shortfin/python/shortfin_apps/sd)
- [sglang-shortfin llama3.1](https://github.com/stbaione/SHARK-Platform/blob/sglang-user-doc/docs/shortfin/llm/user/shortfin_with_sglang_frontend_language.md)
  
# Nightly Test Reports
See latest [CI/Nightly Test Report](https://nod-ai.github.io/shark-ai/). Use [Nod.AI Lab](https://confluence.amd.com/pages/viewpage.action?spaceKey=ENGIT&title=Nod.AI+Lab) page to ssh into machine SharkMi300X to find logs and artifacts to triage the failures. File an issue (if not already filed/listed) and add to Issues table below.

# Issues
| category | issue link | assigned to | status |
|---|---|---|---|
|quark quantization | [QUARK-71](https://jira.xilinx.com/browse/QUARK-71) | Bowen Bow | FP8 matmul should be used in attention|
|iree codegen | [18864](https://github.com/iree-org/iree/issues/18864)| Ian Wood | OOM for 70B |
|iree Negative Memory | [19077](https://github.com/iree-org/iree/issues/19077) | unassigned | op uses -131072 bytes of shared memory

# Schedule
(Model is assumed to be llama3.1 in the following table, e.g. "8B FP8" means "llama3.1 8B FP8 model")
|Item                          | Last Week (Nov 25-27) | Current Week (Dec 2-6) |
|------------------------------|-----------------------|--------------------------|
| Sharktank Modeling           | <br> - @Boian Add CLIP encoder to sharktank (ETA: 11/27) <br> -@Dan fix numeric fp8 issue (ETA: 11/26) | - @Boian CLIP encoder (ETA: 12/5) <br> - @Rob CI llama regression tests (ETA 12/3) <br> - @Ian Finish VAE decode (ETA: 12/5) 
| IREE codegeneration          |- @kunvar support for non deocmposed decode (ETA: 11/27) <br> - @stan: FP8 attention (ETA: 11/27) | 
| Serving | <br> - @stephen / @xida implement Radix Attention in shortfin (ETA: 12/6) <br> - @egarvey wire up Flux.1 in shortfin using ONNX model (ETA:11/27) | <br> - @eagarvey finish Flux pipeline for image generation (ETA: 12/3) <br> - @Stephen Debug CI flakiness (ETA: 12/2) <br> - @Xida landing PR's for attention changes (ETA: 12/3)
| Test Automation              |<br>- @Avi Work with codegen folks to get 405B FP16 fixed and tested (ETA: 11/18) | - @Avi benchmarking dashboard (ETA: 12/3) <br> - @Archana shortfin regression tests (ETA: 12/3)
| Performance Tuning           | | 


# Status-Numerics 
Following naming convention should be used for weights and artifacts (on SharkMI300x and other similar machines)

**UnSharded Weights:** 

/data/<model_name>/weights/<model_size>/<modelname_modelsize_datatype>.irpa

Example: /data/llama-3.1/weights/405b/fp16/llama3.1_405b_fp16.irpa

**Sharded Weights:** 

/data/<model_name>/weights/<model_size>/<shard_size>/<modelname_modelsize_shardsize_ranksuffix>.irpa

Example: /data/llama-3.1/weights/405b/fp16/tp8/llama3.1_405b_fp16_tp8_parameters.rank0.irpa

**Artifacts:** 

/data/<model_name>/artifacts/<model_size>/<model_name>\_<model_size>\_<data_type>\_<attention_kind>\_<sharding>\_<batch_size>.[mlir | vmfb]

Example: /data/llama-3.1/artifacts/405b/llama3.1_405b_fp16_nondecomposed_tp8_bs4.mlir

## llama3.1 decomposed
To generate artifacts, on SharkMI300x, follow sharktank [setup instructions](https://gist.github.com/stbaione/be38bfb214d990a4b765804223d6b948), then:
`python -m sharktank.examples.export_paged_llm_v1 --irpa-file=/data/llama-3.1/weights/8b/fp16/llama3.1_8b_fp16.irpa --output-mlir f16_dc.mlir  --bs=1  --attention-kernel=decomposed`

## llama3.1 405B TP8 commands
1. Shard irpa file:

`
python3 -m sharktank.examples.sharding.shard_llm_dataset --irpa-file llama3_405b_f16.irpa --output-irpa test.irpa --tensor-parallelism-size 8
`

2. Export to MLIR:

Prefill Nondecomposed

`
python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=test.irpa --output-mlir=405b_f16_tp8_decomposed.mlir --output-config=405b_f16_tp8_decomposed.json --bs=4 --attention-kernel torch --skip-decode
`

Prefill + Decode Nondecomposed

`
python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file=test.irpa --output-mlir=405b_f16_tp8_decomposed.mlir --output-config=405b_f16_tp8_decomposed.json --bs=4 --attention-kernel torch
`

3. Compile (FAIL [compile error](https://gist.github.com/aviator19941/73468660ecef16b03b37e9afa2e6d075), seems related to this [PR](https://github.com/iree-org/iree/pull/18663)):

`
iree-compile 405b_f16_tp8_decomposed.mlir --iree-hip-target=gfx942 --iree-hal-target-backends=rocm -o=405b_f16_tp8_decomposed.vmfb --iree-hal-target-device=hip[0] --iree-hal-target-device=hip[1] --iree-hal-target-device=hip[2] --iree-hal-target-device=hip[3] --iree-hal-target-device=hip[4] --iree-hal-target-device=hip[5] --iree-hal-target-device=hip[6] --iree-hal-target-device=hip[7] --iree-dispatch-creation-enable-aggressive-fusion=true --iree-global-opt-propagate-transposes=true --iree-opt-aggressively-propagate-transposes=true --iree-opt-data-tiling=false --iree-preprocessing-pass-pipeline='builtin.module(util.func(iree-preprocessing-generalize-linalg-matmul-experimental))' --iree-hal-indirect-command-buffers=true --iree-stream-resource-memory-model=discrete --iree-hip-legacy-sync=false --iree-hal-memoization=true --iree-opt-strip-assertions
`

(MI300X GPU, SPX Mode)
|Item                                      | Generate MLIR | Compile to vmfb | IREE invocation | IREE numeric [Perplexity](https://github.com/nod-ai/shark-ai/tree/main/sharktank/sharktank/evaluate) | Serving numeric |
|------------------------------------------|---------------|-----------------|-----------------|--------------|-----------------|
| llama3.1-8B-FP16-decomposed      |PASS [TP1 mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_8b/8b_f16_decomposed_11_22.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_f16.gguf) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/8b_f16.irpa)  |PASS [vmfb](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/8b_f16.vmfb) | PASS |14.99 | tbd
| llama3.1-8B-FP16-decomposed-TP8 | PASS ([MLIR](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama3.1_8b_fp16_decomposed_tp8.mlir)) | PASS | PASS | FAIL (probably)  | tbd
| llama3.1-70B-FP16-decomposed      |PASS [TP1 mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_f16.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_f16.gguf) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/70b_f16.irpa) |PASS [vmfb](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/70b_f16.vmfb) | FAIL [OOM](https://github.com/iree-org/iree/issues/18864)  | tbd | tbd
| llama3.1-405B-FP16-decomposed  |PASS [TP1 mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_405b/llama3.1_405b_fp16_TP1.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_405b/llama405b_fp16.gguf)   | tbd | tbd | tbd | tbd
| llama3.1-405B-FP16-decomposed-TP8 | PASS [MLIR](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_405b/llama3.1_405b_f16_tp8.mlir) | PASS [vmfb](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_405b/llama3.1_405b_f16_tp8.vmfb) | FAIL [Registers](https://github.com/iree-org/iree/issues/18923)  | tbd | tbd
| llama3.1-8B-FP8-decomposed   |PASS [TP1 mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_8b.mlir) [irpa](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_8b.irpa) | Fails in iree, [patch](https://github.com/iree-org/iree/pull/18890) | tbd | tbd | tbd
| llama3.1-70B-FP8-decomposed  |PASS [TP1 mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_70b.mlir) [irpa](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_70b.irpa) |Fails in iree, [patch](https://github.com/iree-org/iree/pull/18890) | tbd | tbd | tbd
| llama3.1-405B-FP8-decomposed  | tbd | tbd | tbd | tbd | tbd

## llama3.1 non-decomposed 
(MI300X GPU, SPX Mode)
|Item                                      | Generate MLIR | Compile to vmfb | IREE invocation | IREE numeric | Serving numeric |
|------------------------------------------|---------------|-----------------|-----------------|--------------|-----------------|
| llama3.1-8B-FP16 bs4 TP1 (prefill)     |PASS [mlir_tp1](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_8b/8b_f16_nondecomposed_prefill_12_3.mlir) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/8b_f16.irpa)  | PASS [compile command](https://gist.github.com/aviator19941/d49e0baca5c6f64a93f8926f99cd111f) | PASS [run command](https://gist.github.com/aviator19941/8cc7b1282e69ec87fc4c5e231d28b80f) [numpy inputs](https://gist.github.com/aviator19941/fe1371e45dc5d28651145884df0b314b) | tbd | tbd
| llama3.1-8B-FP16 bs4 TP8 (prefill)     |PASS [mlir_tp8](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_8b/8b_f16_bs4_tp8_nondecomposed_prefill.mlir)  | PASS w/ [#19379](https://github.com/iree-org/iree/pull/19379) + [#19381](https://github.com/iree-org/iree/pull/19381) | tbd | tbd | tbd
| llama3.1-8B-FP16      |PASS [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/8b_f16_nondecomposed.mlir)   | Fails in iree, [patch](https://github.com/iree-org/iree/pull/18890) | tbd | tbd | tbd
| llama3.1-70B-FP16      |PASS [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/70b_f16_nondecomposed.mlir)   |Fails in iree, [patch](https://github.com/iree-org/iree/pull/18890) | tbd | tbd | tbd
| llama3.1-405B-FP16  |  PASS [mlir_tp8](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_405b/405b_f16_tp8_nondecomposed_bs4.mlir) | PASS | FAIL [OOM](https://github.com/iree-org/iree/issues/18864) | tbd | tbd
| llama3.1-8B-FP8   |PASS [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/f8_ndc.mlir)    | tbd | tbd | tbd | tbd
| llama3.1-70B-FP8  |ETA: 11/1   | tbd | tbd | tbd | tbd
| llama3.1-405B-FP8 |ETA: 11/5   | tbd | tbd | tbd | tbd
| llama-toy-size-FP32-TP2-CPU | PASS | PASS | tbd | tbd | tbd

## llama3.1 decodeposed 
(only decode is decomposed)
(MI300X GPU, SPX Mode)
|Item                                      | Generate MLIR | Compile to vmfb | IREE invocation | IREE numeric | Serving numeric |
|------------------------------------------|---------------|-----------------|-----------------|--------------|-----------------|
| llama3.1-8B-FP16      |PASS [mlir_tp1](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_8b/llama8b_f16_tp1_decodeposed_bs4.mlir)   | tbd | tbd | tbd | tbd
| llama3.1-70B-FP16      | tbd | tbd | tbd | tbd | tbd
| llama3.1-405B-FP16  |PASS [mlir_tp8](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_405b/llama3_405b_f16_tp8_decodeposed.mlir)    | tbd | tbd | tbd | tbd
| llama3.1-8B-FP8   | PASS [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/f8_half_ndc.mlir) | Fail (attention, Dan currently looking into this) | tbd | tbd | tbd
| llama3.1-70B-FP8  | tbd  | tbd | tbd | tbd | tbd
| llama3.1-405B-FP8  | tbd | tbd | tbd | tbd | tbd
| llama-toy-size-FP32-TP2-CPU  | tbd | tbd | tbd | tbd | tbd

## Flux.1 dev
|Item              | Generate MLIR | Compile to vmfb | IREE invocation | IREE numeric | Serving numeric |
|------------------|---------------|-----------------|-----------------|--------------|-----------------|
| Flux1.dev ONNX   |tbd | tbd | tbd | tbd | tbd

### T5 Encoder (part of Flux.1 dev)

Only the `xxl` variant is actually used in FLUX. The `small` variant is provided for faster iteration if needed.

#### Compile
```bash
iree-compile \
  google__t5_v1_1_xxl_encoder_fp32.mlir \
  --iree-hal-target-device=hip \
  --iree-hip-target=gfx942 \
  -o google__t5_v1_1_xxl_encoder_fp32.vmfb
```

### Run
```bash
iree-run-module \
  --device=hip \
  --module=google__t5_v1_1_xxl_encoder_fp32.vmfb \
  --parameters=model=google__t5_v1_1_xxl_encoder_fp32.irpa \
  --function=forward_bs4 \
  --input=@google__t5_v1_1_xxl_iree_forward_bs4_arg0.npy
```

(MI300X GPU, SPX Mode)
|Item                                      | Generate MLIR | Compile to vmfb | IREE invocation | IREE numeric | Serving numeric |
|------------------------------------------|---------------|-----------------|-----------------|--------------|-----------------|
| t5-v1.1-small-encoder-bf16      |PASS [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_bf16.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5-v1_1-small_bf16.gguf) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_bf16.irpa)  | PASS | PASS [args](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_bf16_iree_forward_bs4_arg0.npy) [expected_result](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_bf16_torch_forward_result0.npy) | FAIL | tbd
| t5-v1.1-xxl-encoder-bf16      |PASS [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_bf16.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5-v1_1-xxl_bf16.gguf) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_bf16.irpa)  | PASS | PASS [args](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_bf16_iree_forward_bs4_arg0.npy) [expected_result](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_bf16_torch_forward_result0.npy) | FAIL | tbd
| t5-v1.1-small-encoder-f32      |PASS [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_f32.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5-v1_1-small_f32.gguf) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_f32.irpa)  | PASS | PASS [args](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_f32_iree_forward_bs4_arg0.npy) [expected_result](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/small/google__t5_v1_1_small_encoder_f32_torch_forward_result0.npy) | PASS `tol < (atol=1e-4, rtol=1.5e-3)` | tbd
| t5-v1.1-xxl-encoder-f32      |PASS [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_f32.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5-v1_1-xxl_f32.gguf) [irpa](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_f32.irpa)  | PASS | PASS [args](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_f32_iree_forward_bs4_arg0.npy) [expected_result](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/t5/xxl/google__t5_v1_1_xxl_encoder_f32_torch_forward_result0.npy) | PASS `tol < (atol=1e-4, rtol=1.5e-3)` | tbd

## Mixtral 8x7B
|Item              | Generate MLIR | Compile to vmfb | IREE invocation | IREE numeric | Serving numeric |
|------------------|---------------|-----------------|-----------------|--------------|-----------------|
| Mixtral 8x7B ONNX   |tbd | tbd | tbd | tbd | tbd


# AMD GPU Machines
[MI300](https://confluence.amd.com/display/ENGIT/Nod.AI+Lab#Nod.AILab-MI300NodAIMachines)


## MLIR generation and Compilation
Generate IR
```
python3 -m sharktank.examples.export_paged_llm_v1 --irpa-file <input_irpa path with correct sharding and dtype> --output-mlir <output-mlir> --bs <batch size> --tensor-parallelism-size <TP size if sharding> --attention-kernel <decomposed or torch_sdpa> --no-fake-quant <only for fp8>
```
Generate vmfb
```
iree-compile --iree-hal-target-backends=rocm --iree-hip-target=gfx942 -o <output-vmfb path>
```

## Evaluation tests
### Perplexity
Follow the steps [here](https://github.com/nod-ai/SHARK-Platform/blob/main/sharktank/sharktank/evaluate/README.md)

## Accessing sharkblobs on Azure:
In browser, click on [sharkblobs](https://portal.azure.com/#@amdcloud.onmicrosoft.com/resource/subscriptions/8c190d1b-eb91-48d5-bec5-3e7cb7412e6c/resourceGroups/pdue-nod-ai-rg/providers/Microsoft.Storage/storageAccounts/sharkblobs/storagebrowser) , then click on "Blob-containers" and the click on "halo-models"

Or, use command line by first installing az cli as:
```
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
```
And then, get the account key for the storage account by clicking on "Storage Accounts" in Azure Services or searching "sharkblobs" in the top search bar. Then, click on sharkblobs. Then, on the left side bar, under Security + networking, click on "Access keys". Copy the account key from here and use in the following command
To upload:
```
az storage blob upload --account-name sharkblobs --container-name halo-models --name <azure path, example: halo-models/llama3_8b/tp1/llama.mlir> --file <local_path_on_computer> --account-key <key_retrieved_from_directions_above>
```

To download:
```
az storage blob download --account-name sharkblobs --container-name halo-models --name <azure path, example: halo-models/llama3_8b/tp1/llama.mlir> --file <local_path_on_computer> --account-key <key_retrieved_from_directions_above>
```

if you are downloading from "sharkpublic" then replace instructions above by sharkpublic and get your account access key for sharkpublic.
Example:
```
az storage blob download --account-name sharkpublic --container-name sharkpublic --name ian/llama8b_f16.gguf --file llama8b_f16.gguf --account-key <key string>
```

## Export With `sharktank` and Server with `shortfin`:

Follow the steps [here](https://github.com/nod-ai/shark-ai/blob/main/docs/shortfin/llm/user/e2e_llama8b_mi300x.md).

## Setup SGLang With Shortfin

Follow the steps [here](https://github.com/nod-ai/shark-ai/blob/main/docs/shortfin/llm/user/shortfin_with_sglang_frontend_language.md)

## SGLang/Shortfin Feature Enablement

| Feature     | Description | Enabled    | Enablement Requirements | Reference(s) |
| ----------- | ----------- | ---------- | ----------------------- | ------------ |
| `gen`       | Generate shortfin completion, given a prompt | Yes | Enabled | [Shortfin Implementation](https://github.com/stbaione/sglang/blob/main/python/sglang/lang/backend/shortfin.py#L47) |
| `streaming` | Stream shortfin completion, given a prompt | Yes | Enabled | [Shortfin Implementation](https://github.com/stbaione/sglang/blob/main/python/sglang/lang/backend/shortfin.py#L68) |
| `run_batch` | Run batch of disjoint requests with continous batching | Yes | Enabled | [Batch Docs](https://sgl-project.github.io/frontend/frontend.html#batching) |
| `fork`      | Launch parallel prompts | Yes | Enabled | [Fork Docs](https://sgl-project.github.io/frontend/frontend.html#parallelism) |
| `choices`   | Given set of choices, generate response based on best log probs | No | Should work with greedy. Needs backend implementation | [Greedy Token Selection](https://sgl-project.github.io/references/choices_methods.html#greedy-token-selection) [OpenAI Implementation](https://github.com/sgl-project/sglang/blob/main/python/sglang/lang/backend/openai.py#L295) |
| `image`     | Pass image as part of multi-modal prompt | No | Multi-Modal not supported by SF | [sgl.image Docs](https://sgl-project.github.io/frontend/frontend.html#multi-modality) |
| `regex`     | Specify regular expression as decoding constraint | No | Only supported for local models | [Regex Docs](https://sgl-project.github.io/frontend/frontend.html#constrained-decoding) |

## SGLang Benchmark Results

The latest benchmark results for the SGLang integration can be found [here](https://amdcloud-my.sharepoint.com/:x:/g/personal/stbaione_amd_com/EX255KFtxHRMqTW8rBQTmuEBkC4hA8PVYpMTgeOtsxiYxQ?e=VajHh4)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Archive
## Status (Old)
(Note: Do not update this one)
|Models | compile | inference (SPX mode) | tracy |
|---|---|---|---|
|llama3.1-8b-Q4_1| PASS | prefill (1817 ms), decode (57.3 ms), [commands](https://gist.github.com/aviator19941/f10b5b7a7c3975de4363450b4d7ec68f) | [prefill](https://sharkpublic.blob.core.windows.net/sharkpublic/avi/llama8b_q4_1_prefill_v2.tracy) [decode](https://sharkpublic.blob.core.windows.net/sharkpublic/avi/llama8b_q4_1_decode_v2.tracy) |
|llama3.1-8b-Q4_k| PASS | | |
|llama3.1-70b-Q4_1| PASS | prefill (3543 ms), decode (213 ms), [commands](https://gist.github.com/aviator19941/79ee5afc39c225ec7469030320014fa3) | [prefill](https://sharkpublic.blob.core.windows.net/sharkpublic/avi/llama70b_q4_1_prefill.tracy) [decode](https://sharkpublic.blob.core.windows.net/sharkpublic/avi/llama70b_q4_1_decode.tracy) |
|grok-1-Q4_1| PASS | FAIL, out of memory | [prefill](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/grok-1/grok-1-q4_1-rocm-prefill.tracy) [decode](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/grok-1/grok-1-q4_1-rocm-decode.tracy) |

## Artifacts (Old)
(Note: Update Schedule-Numerics table for llama3.1 artifacts instead of this table (10/20/2024 onwards))
### Guideline:
1) small files and MLIR files check into [llm-dev](https://github.com/nod-ai/llm-dev)
2) large files upload to [sharkblobs](https://portal.azure.com/#@amdcloud.onmicrosoft.com/resource/subscriptions/8c190d1b-eb91-48d5-bec5-3e7cb7412e6c/resourceGroups/pdue-nod-ai-rg/providers/Microsoft.Storage/storageAccounts/sharkblobs/storagebrowser) -> "halo-models" container on Azure and put link to that in the table(s) below
3) Very large files, store on GPU server and note the name/location of/on the machine in table(s) below 

Note: If a link to Azure sharkblob below gives you an error, either use az cli to download (see section Accessing sharkblobs on Azure) or click on [sharkblobs](https://portal.azure.com/#@amdcloud.onmicrosoft.com/resource/subscriptions/8c190d1b-eb91-48d5-bec5-3e7cb7412e6c/resourceGroups/pdue-nod-ai-rg/providers/Microsoft.Storage/storageAccounts/sharkblobs/storagebrowser) , then click on "Blob containers" and then navigate to the file manually and download it. 

### TP1 

Models           |     FP16        |   FP8           |     Q4_1         |    Q4_K       |    Attention IRs
:--------------: | :-------------: |:----------------:|:---------------:|:-------------:|:------------------:
llama2-7b | | [irpa](https://sharkblobs.blob.core.windows.net/dan/qdq_full_transpose.irpa) [mlir](https://sharkblobs.blob.core.windows.net/dan/batch_llama_v1.mlir) | | | [Attention IRs](https://github.com/nod-ai/llm-dev/tree/main/models/llama_attention_irs)
llama3-8b | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_f16.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_f16.gguf) | [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_8b.mlir) [irpa](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_8b.irpa) | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_q4_1.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_q4_1.gguf) | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_q4_k.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_8b/llama8b_Q4_K.gguf) |
llama3-70b | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_f16.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_f16.gguf) |  [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_70b.mlir) [irpa](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/native_fp8_e4m3fnuz_llama3_8_70b.irpa)| [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_q4_1.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_q4_1.gguf) | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_q4_k.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_70b/llama70b_q4_k.gguf) |
llama3-405b | [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/llm-dev/llama3_405b/llama3.1_405b_fp16_TP1.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_405b/llama405b_fp16.gguf) | | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_405b/llama405b_q4_1.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_405b/llama405b_q4_1.gguf) | [mlir](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_405b/llama405b_q4_k.mlir) [gguf](https://sharkblobs.blob.core.windows.net/halo-models/llm-dev/llama3_405b/llama405b_q4_k.gguf) |
grok-1 | [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/dan/grok.mlir) [gguf](https://sharkpublic.blob.core.windows.net/sharkpublic/llm-dev/grok_1/grok-1-f16.gguf) |NA | [mlir](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/grok-1/grok-1-q4_1-irpa.mlir) [gguf](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/grok-1/grok-1-q4_1.gguf) | [gguf](https://sharkpublic.blob.core.windows.net/sharkpublic/halo-models/grok-1/grok-1-q4_k.gguf) |


